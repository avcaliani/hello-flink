package br.avcaliani.hello_flink.pipelines;

import br.avcaliani.hello_flink.cli.Args;
import br.avcaliani.hello_flink.infra.CSV;
import br.avcaliani.hello_flink.infra.Kafka;
import br.avcaliani.hello_flink.infra.serializers.KafkaSerializer;
import br.avcaliani.hello_flink.models.in.Transaction;
import br.avcaliani.hello_flink.models.in.User;
import br.avcaliani.hello_flink.models.out.DTOTransaction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.state.MapStateDescriptor;
import org.apache.flink.connector.base.DeliveryGuarantee;
import org.apache.flink.connector.kafka.sink.KafkaSink;
import org.apache.flink.streaming.api.datastream.BroadcastStream;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.functions.co.BroadcastProcessFunction;
import org.apache.flink.util.Collector;


/**
 * This Pipeline reads the data from a Kafka stream, enrich the data and output to another topic.
 * </br>
 * Here's the guide I used to create this ðŸ‘‰
 * <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/">
 * Doc: Apache Flink + Kafka Connector
 * </a>
 */
public class ValidateTransactions extends Pipeline {

    private static final String KAFKA_OUT_TOPIC = "DONU_EXECUTE_TRANSACTION_V1";
    private static final String KAFKA_DEAD_LETTER = "DLQ_DONU_TRANSACTIONS_V1";

    @Override
    public Pipeline run(Args args) throws Exception {

        var env = args.getEnv();
        var kafka = new Kafka(env, args.getKafkaBrokers());

        var users = new CSV(env)
                .read(args.getBucket() + "/raw/users/", User.class);

        var transactions = kafka.read(
                "DONU_TRANSACTIONS_V1", /* Topic */
                "hello-flink--validate-txn-pipeline", /* Group ID */
                Transaction.class
        );

        var richTxn = joinData(transactions, users)
                .map(isTxnValid());

        // https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/
        var dlqSink = KafkaSink.<DTOTransaction>builder()
                .setBootstrapServers(args.getKafkaBrokers())
                .setRecordSerializer(new KafkaSerializer<>(KAFKA_DEAD_LETTER))
                .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
                .build();

        richTxn.filter(DTOTransaction::isInvalid).sinkTo(dlqSink);

        // Kafka brokers by default have transaction.max.timeout.ms set to 15 minutes.
        // This property will not allow to set transaction timeouts for the producers larger than its value.
        // FlinkKafkaProducer by default sets the transaction.timeout.ms property in producer config to 1 hour,
        // thus transaction.max.timeout.ms should be increased before using the Semantic.EXACTLY_ONCE mode.
        // Ref: https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/datastream/kafka/
        var validSink = KafkaSink.<DTOTransaction>builder()
                .setBootstrapServers(args.getKafkaBrokers())
                .setRecordSerializer(new KafkaSerializer<>(KAFKA_OUT_TOPIC))
                // https://flink.apache.org/2018/02/28/an-overview-of-end-to-end-exactly-once-processing-in-apache-flink-with-apache-kafka-too/
                .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)
                .setProperty("transaction.timeout.ms", "900000")
                .setTransactionalIdPrefix("flink-valid-txn") // Mandatory when using Exactly Once
                .build();
        richTxn.filter(DTOTransaction::isValid).sinkTo(validSink);

        env.execute("hello-flink--validate-transactions");
        return this;
    }

    /**
     * It enriches the transaction data adding the user information for receiver and sender.
     *
     * @param transactions Transactions Stream.
     * @param users        Users Stream.
     * @return Enriched Transaction Data.
     */
    private DataStream<DTOTransaction> joinData(DataStream<Transaction> transactions, DataStream<User> users) {
        var userStateDescriptor = new MapStateDescriptor<>("userBroadcastState", String.class, User.class);
        BroadcastStream<User> broadcastUserStream = users.broadcast(userStateDescriptor);
        // TODO: Explain why I'm using connect and not a window join.
        return transactions
                .connect(broadcastUserStream)
                .process(new EnrichTxnFunction(userStateDescriptor));
    }

    /**
     * Check and update the traction "isValid" field.
     *
     * @return Updated Transaction.
     */
    private MapFunction<DTOTransaction, DTOTransaction> isTxnValid() {
        return txn -> {

            var isMissingUser = txn.getSender() == null || txn.getReceiver() == null;
            var isSameUser = txn.getSender() == txn.getReceiver();
            var isValid = !isMissingUser && !isSameUser;

            txn.setIsValid(isValid);
            if (isValid)
                return txn;

            var errors = txn.getErrors();
            if (isMissingUser)
                errors.add("missing user(s) for transaction (from:" + txn.getFrom() + " -> to:" + txn.getTo() + ").");

            if (isSameUser)
                errors.add("transaction have the same user as sender and receiver.");

            return txn;
        };
    }

    /**
     * This class is the broadcast process function.
     * <p>
     * It will enrich the transaction entity with user entity data.
     * <p>
     * Here's the guide I used to create this ðŸ‘‰
     * <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/broadcast_state/">
     * Doc: Apache Flink - The Broadcast State Pattern
     * </a>
     */
    private static class EnrichTxnFunction extends BroadcastProcessFunction<Transaction, User, DTOTransaction> {

        private final MapStateDescriptor<String, User> descriptor;

        EnrichTxnFunction(MapStateDescriptor<String, User> descriptor) {
            this.descriptor = descriptor;
        }

        @Override
        public void processBroadcastElement(User user, Context ctx, Collector<DTOTransaction> out) throws Exception {
            var userState = ctx.getBroadcastState(this.descriptor);
            userState.put(user.getId(), user);
        }

        @Override
        public void processElement(Transaction txn, ReadOnlyContext ctx, Collector<DTOTransaction> out) throws Exception {
            var userState = ctx.getBroadcastState(this.descriptor);
            User sender = userState.get(txn.getFrom());
            User receiver = userState.get(txn.getTo());
            out.collect(new DTOTransaction(txn, sender, receiver));
        }

    }
}
